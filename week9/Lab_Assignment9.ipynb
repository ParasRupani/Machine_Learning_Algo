{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment 9\n",
    "#### Student Name: Paras Rupani\n",
    "#### ID: 8961758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary Packages\n",
    "import string, math\n",
    "\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.tokenize import  word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample sentences above. You are required for this assignment to implement four functions **from scratch**. <br>\n",
    "You are required to preprocess the text and apply the tokenization process as done in assignment 8. (3)\n",
    "***THEN***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocess(input_text):\n",
    "    special_chars = set(string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # POS Tagging\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "\n",
    "        elif tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "            \n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    # Tokenizing the input\n",
    "    tokens = word_tokenize(input_text)\n",
    "\n",
    "    # Removing stopwords and punctuation\n",
    "    clean_tokens = [token for token in tokens if token.lower() not in stop_words and token not in special_chars]\n",
    "\n",
    "    # Implementing Part-of-speech tagging   \n",
    "    pos_tags = pos_tag(clean_tokens)\n",
    "\n",
    "    # Lemmatization with POS tags\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Python', 'versatile', 'programming', 'language'],\n",
       " ['JavaScript', 'widely', 'use', 'web', 'development'],\n",
       " ['Java', 'know', 'platform', 'independence'],\n",
       " ['Programming', 'involves', 'write', 'code', 'solve', 'problem'],\n",
       " ['Data', 'structure', 'crucial', 'efficient', 'program'],\n",
       " ['Algorithms', 'step-by-step', 'instruction', 'solve', 'problem'],\n",
       " ['Version',\n",
       "  'control',\n",
       "  'system',\n",
       "  'help',\n",
       "  'manage',\n",
       "  'code',\n",
       "  'change',\n",
       "  'collaboration'],\n",
       " ['Debugging', 'process', 'find', 'fix', 'error', 'code'],\n",
       " ['Web', 'frameworks', 'simplify', 'development', 'web', 'application'],\n",
       " ['Artificial', 'intelligence', 'apply', 'various', 'programming', 'task']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = []\n",
    "for sentence in sample_sentences:\n",
    "    output.append(nlp_preprocess(sentence))\n",
    "\n",
    "print(\"Preprocessed Text:\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now tokenizing the preprocessed data\n",
    "\n",
    "input_sentences = []\n",
    "for sentence in output:\n",
    "    # Joining the tokens into a string\n",
    "    sentence_string = ' '.join(sentence)\n",
    "    # Tokenizing the sentence\n",
    "    tokenize = word_tokenize(sentence_string)\n",
    "    # Removing punctuation and converting it into lower case\n",
    "    remove_punctuation = [word.lower() for word in tokenize if word.isalpha()]\n",
    "    # Removing stop words\n",
    "    stop_words_tokens = [word for word in remove_punctuation if word not in stopwords.words('english')]\n",
    "    input_sentences.append(stop_words_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text:\n",
      "['python', 'versatile', 'programming', 'language']\n",
      "['javascript', 'widely', 'use', 'web', 'development']\n",
      "['java', 'know', 'platform', 'independence']\n",
      "['programming', 'involves', 'write', 'code', 'solve', 'problem']\n",
      "['data', 'structure', 'crucial', 'efficient', 'program']\n",
      "['algorithms', 'instruction', 'solve', 'problem']\n",
      "['version', 'control', 'system', 'help', 'manage', 'code', 'change', 'collaboration']\n",
      "['debugging', 'process', 'find', 'fix', 'error', 'code']\n",
      "['web', 'frameworks', 'simplify', 'development', 'web', 'application']\n",
      "['artificial', 'intelligence', 'apply', 'various', 'programming', 'task']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized Text:\")\n",
    "for sentence in input_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the inverted index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the inverted index\n",
    "\n",
    "    inverted_token_index = {}\n",
    "\n",
    "    # Iterate over each sentence with its index\n",
    "    for token_sentence_id, token_sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "        # Iterate over tokens in the current sentence\n",
    "        for token in token_sentence:\n",
    "            # Add the sentence ID to the token's list if token is already in the inverted index\n",
    "            if token in inverted_token_index:\n",
    "                if token_sentence_id not in inverted_token_index[token]:\n",
    "                    inverted_token_index[token].append(token_sentence_id)\n",
    "            else:\n",
    "                # Initialize a new list for the token if it's not in the inverted index\n",
    "                inverted_token_index[token] = [token_sentence_id]\n",
    "    \n",
    "    return inverted_token_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': [1],\n",
       " 'versatile': [1],\n",
       " 'programming': [1, 4, 10],\n",
       " 'language': [1],\n",
       " 'javascript': [2],\n",
       " 'widely': [2],\n",
       " 'use': [2],\n",
       " 'web': [2, 9],\n",
       " 'development': [2, 9],\n",
       " 'java': [3],\n",
       " 'know': [3],\n",
       " 'platform': [3],\n",
       " 'independence': [3],\n",
       " 'involves': [4],\n",
       " 'write': [4],\n",
       " 'code': [4, 7, 8],\n",
       " 'solve': [4, 6],\n",
       " 'problem': [4, 6],\n",
       " 'data': [5],\n",
       " 'structure': [5],\n",
       " 'crucial': [5],\n",
       " 'efficient': [5],\n",
       " 'program': [5],\n",
       " 'algorithms': [6],\n",
       " 'instruction': [6],\n",
       " 'version': [7],\n",
       " 'control': [7],\n",
       " 'system': [7],\n",
       " 'help': [7],\n",
       " 'manage': [7],\n",
       " 'change': [7],\n",
       " 'collaboration': [7],\n",
       " 'debugging': [8],\n",
       " 'process': [8],\n",
       " 'find': [8],\n",
       " 'fix': [8],\n",
       " 'error': [8],\n",
       " 'frameworks': [9],\n",
       " 'simplify': [9],\n",
       " 'application': [9],\n",
       " 'artificial': [10],\n",
       " 'intelligence': [10],\n",
       " 'apply': [10],\n",
       " 'various': [10],\n",
       " 'task': [10]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_token_index = get_inverted_index(input_sentences)\n",
    "inverted_token_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the Positional index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the positional index\n",
    "\n",
    "    positional_token_index = {}\n",
    "\n",
    "    # Using for loop for iterate over each sentence with its index\n",
    "    for token_sentence_id_1, token_sentence_1 in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for tokens_position, tokens_1 in enumerate(token_sentence_1):\n",
    "            # Using the IF condition to add the token to the positional index if it is not already there.\n",
    "            if tokens_1 not in positional_token_index:\n",
    "                positional_token_index[tokens_1] = {}\n",
    "\n",
    "            # Now adding the sentence ID if it isn't already there in the token's entry.\n",
    "            if token_sentence_id_1 not in positional_token_index[tokens_1]:\n",
    "                positional_token_index[tokens_1][token_sentence_id_1] = []\n",
    "                \n",
    "            positional_token_index[tokens_1][token_sentence_id_1].append(tokens_position)\n",
    "\n",
    "    return positional_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': {1: [0]},\n",
       " 'versatile': {1: [1]},\n",
       " 'programming': {1: [2], 4: [0], 10: [4]},\n",
       " 'language': {1: [3]},\n",
       " 'javascript': {2: [0]},\n",
       " 'widely': {2: [1]},\n",
       " 'use': {2: [2]},\n",
       " 'web': {2: [3], 9: [0, 4]},\n",
       " 'development': {2: [4], 9: [3]},\n",
       " 'java': {3: [0]},\n",
       " 'know': {3: [1]},\n",
       " 'platform': {3: [2]},\n",
       " 'independence': {3: [3]},\n",
       " 'involves': {4: [1]},\n",
       " 'write': {4: [2]},\n",
       " 'code': {4: [3], 7: [5], 8: [5]},\n",
       " 'solve': {4: [4], 6: [2]},\n",
       " 'problem': {4: [5], 6: [3]},\n",
       " 'data': {5: [0]},\n",
       " 'structure': {5: [1]},\n",
       " 'crucial': {5: [2]},\n",
       " 'efficient': {5: [3]},\n",
       " 'program': {5: [4]},\n",
       " 'algorithms': {6: [0]},\n",
       " 'instruction': {6: [1]},\n",
       " 'version': {7: [0]},\n",
       " 'control': {7: [1]},\n",
       " 'system': {7: [2]},\n",
       " 'help': {7: [3]},\n",
       " 'manage': {7: [4]},\n",
       " 'change': {7: [6]},\n",
       " 'collaboration': {7: [7]},\n",
       " 'debugging': {8: [0]},\n",
       " 'process': {8: [1]},\n",
       " 'find': {8: [2]},\n",
       " 'fix': {8: [3]},\n",
       " 'error': {8: [4]},\n",
       " 'frameworks': {9: [1]},\n",
       " 'simplify': {9: [2]},\n",
       " 'application': {9: [5]},\n",
       " 'artificial': {10: [0]},\n",
       " 'intelligence': {10: [1]},\n",
       " 'apply': {10: [2]},\n",
       " 'various': {10: [3]},\n",
       " 'task': {10: [5]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_token_index = get_positional_index(input_sentences)\n",
    "positional_token_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the TF-IDF Matrix that is sufficient to represent the documents. Assume that each sentence is a document and the sentence ID starts from 1. (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the tf-idf matrix\n",
    "    \n",
    "    # Firstly, flattening the list to find the unique terms\n",
    "    tfidf_unique_terms = set(term for sentence in list_of_sentence_tokens for term in sentence)\n",
    "    tfidf_num_docs = len(list_of_sentence_tokens)\n",
    "\n",
    "    # After that creating a function to calculate TF-IDF term frequency\n",
    "    def tfidf_term_frequency(term, sentence):\n",
    "        return sentence.count(term) / len(sentence)\n",
    "\n",
    "    # Now creating another function to calculate inverse document frequency\n",
    "    def tfidf_inverse_document_frequency(term, list_of_docs):\n",
    "        count = sum(term in doc for doc in list_of_docs)\n",
    "        return math.log(tfidf_num_docs / (1 + count))\n",
    "\n",
    "    # Creating TF-IDF matrix (rows: sentences, columns: unique terms)\n",
    "    tfidf_matrix = []\n",
    "\n",
    "    for tfidf_sentence in list_of_sentence_tokens:\n",
    "        sentence_tfidf = []\n",
    "        for term in tfidf_unique_terms:\n",
    "            tf = tfidf_term_frequency(term, tfidf_sentence)\n",
    "            idf = tfidf_inverse_document_frequency(term, list_of_sentence_tokens)\n",
    "            tfidf = tf * idf\n",
    "            sentence_tfidf.append(tfidf)\n",
    "        tfidf_matrix.append(sentence_tfidf)\n",
    "\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "Sentence\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t16\t17\t18\t19\t20\t21\t22\t23\t24\t25\t26\t27\t28\t29\t30\t31\t32\t33\t34\t35\t36\t37\t38\t39\t40\t41\t42\t43\t44\t45\n",
      "1\t\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.402\t0.0\t0.0\t0.402\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.402\t0.0\t0.0\t0.229\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "2\t\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.322\t0.0\t0.0\t0.322\t0.241\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.241\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.322\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "3\t\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.402\t0.402\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.402\t0.0\t0.0\t0.0\t0.402\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "4\t\t0.0\t0.0\t0.201\t0.0\t0.268\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.153\t0.0\t0.0\t0.0\t0.201\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.268\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.153\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "5\t\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.322\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.322\t0.0\t0.0\t0.0\t0.322\t0.0\t0.0\t0.322\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.322\t0.0\n",
      "6\t\t0.0\t0.0\t0.301\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.301\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.402\t0.402\t0.0\t0.0\t0.0\t0.0\n",
      "7\t\t0.0\t0.201\t0.0\t0.201\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.201\t0.0\t0.201\t0.0\t0.115\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.201\t0.0\t0.0\t0.0\t0.201\t0.0\t0.0\t0.0\t0.0\t0.201\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "8\t\t0.268\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.268\t0.0\t0.268\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.153\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.268\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.268\n",
      "9\t\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.268\t0.0\t0.401\t0.0\t0.0\t0.0\t0.268\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.201\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.268\t0.0\t0.0\t0.0\n",
      "10\t\t0.0\t0.0\t0.0\t0.0\t0.0\t0.268\t0.0\t0.268\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.268\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.268\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.153\t0.0\t0.0\t0.0\t0.0\t0.268\t0.0\t0.0\n"
     ]
    }
   ],
   "source": [
    "# Print the TF-IDF Matrix header\n",
    "print(\"TF-IDF Matrix:\")\n",
    "\n",
    "# Print the column headers with index of each sentence\n",
    "print(\"Sentence\", *range(1, len(get_TFIDF_matrix(input_sentences)[0]) + 1), sep=\"\\t\")\n",
    "\n",
    "# Loop through each sentence in the TF-IDF matrix\n",
    "for sentence_id, sentence in enumerate(get_TFIDF_matrix(input_sentences), start=1):\n",
    "    # Calculate TF-IDF values for the current sentence and format them\n",
    "    tf_sentence = '\\t'.join([f\"{round(value, 3)}\" for value in sentence])\n",
    "    \n",
    "    # Print the sentence index and its corresponding TF-IDF values\n",
    "    print(f\"{sentence_id}\\t\\t{tf_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 4: Create a method that takes as an input: (10)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    # TODO: Implement the functionality that returns the rank of the documents based on the method given and the search query\n",
    "    ## If the method is \"inverted\" then rank the documents based on the number of matching tokens \n",
    "    ## If the method is \"tfidf\" then use the tfidf score equation in slides and return ranking based on the score\n",
    "    ## The document with highest relevance should be ranked first\n",
    "    ## list method should return the index of the documents based on highest ranking first\n",
    "    rank_list = []\n",
    "\n",
    "    if method_name == \"inverted\":\n",
    "        # Creating an inverted index\n",
    "        inverted_index = {}\n",
    "\n",
    "        # Iterate through each sentence and tokenize it\n",
    "        for inverted_sentence_id, inverted_sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "            # Iterate through each token in the sentence\n",
    "            for token in inverted_sentence:\n",
    "                # If token not in the inverted index, add it\n",
    "                if token not in inverted_index:\n",
    "                    inverted_index[token] = []\n",
    "                # If the sentence ID is not already in the token's list, add it\n",
    "                if inverted_sentence_id not in inverted_index[token]:\n",
    "                    inverted_index[token].append(inverted_sentence_id)\n",
    "\n",
    "        # Now, ranking the documents based on the number of matching tokens\n",
    "        rank_query_tokens = search_query.split()  # Splitting the search query into tokens\n",
    "        doc_scores = {}  # Dictionary to store document scores\n",
    "        for token in rank_query_tokens:  # Looping through each token in the search query\n",
    "            if token in inverted_index:  # Checking if the token exists in the inverted index\n",
    "                for doc_id in inverted_index[token]:  # Looping through the documents containing the token\n",
    "                    if doc_id not in doc_scores:  # If the document is not yet scored\n",
    "                        doc_scores[doc_id] = 0  # Initialize the score to 0\n",
    "                    doc_scores[doc_id] += 1  # Increment the score for the document\n",
    "\n",
    "\n",
    "        rank_list = sorted(doc_scores, key=doc_scores.get, reverse=True)\n",
    "\n",
    "    elif method_name == \"TF-IDF\":\n",
    "        import math\n",
    "\n",
    "        # Creating the TF-IDF matrix\n",
    "        def tfidf_term_frequency(term, sentence):\n",
    "            return sentence.count(term) / len(sentence)\n",
    "\n",
    "        def tfidf_inverse_document_frequency(term, list_of_docs):\n",
    "            count = sum(term in doc for doc in list_of_docs)\n",
    "            return math.log(len(list_of_sentence_tokens) / (1 + count))\n",
    "\n",
    "        # After that converting unique_terms from set to list to use 'index' method\n",
    "        tfidf_unique_terms = list(set(term for sentence in list_of_sentence_tokens for term in sentence))\n",
    "\n",
    "        # Creating the TF-IDF matrix\n",
    "        tfidf_matrix = []\n",
    "        for sentence in list_of_sentence_tokens:\n",
    "            sentence_tfidf = []\n",
    "            # Calculating TF-IDF for each term in the sentence\n",
    "            for term in tfidf_unique_terms:\n",
    "                # Calculating term frequency (TF) for the term in the sentence\n",
    "                tf = tfidf_term_frequency(term, sentence)\n",
    "                # Calculating inverse document frequency (IDF) for the term\n",
    "                idf = tfidf_inverse_document_frequency(term, list_of_sentence_tokens)\n",
    "                # Calculating TF-IDF score for the term in the sentence\n",
    "                tfidf = tf * idf\n",
    "                sentence_tfidf.append(tfidf)\n",
    "            tfidf_matrix.append(sentence_tfidf)\n",
    "\n",
    "\n",
    "        # Ranking the documents based on the TF-IDF score\n",
    "        rank_query_tokens = set(search_query.split())  # Extract unique tokens from the search query\n",
    "        doc_scores = {}\n",
    "        for doc_id, sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "            score = sum(tfidf_matrix[doc_id - 1][tfidf_unique_terms.index(token)] for token in rank_query_tokens if token in tfidf_unique_terms)\n",
    "            doc_scores[doc_id] = score  # Assign TF-IDF score to each document\n",
    "\n",
    "        rank_list = sorted(doc_scores, key=doc_scores.get, reverse=True)  # Sort documents by TF-IDF score in descending order\n",
    "\n",
    "\n",
    "    return rank_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Sample Word: development\n",
      "Sentence: 2\n",
      "Sentence: 9\n"
     ]
    }
   ],
   "source": [
    "# Using Inverted method for a sample query\n",
    "sample_query = \"development\"  # Define the sample query\n",
    "method_name = \"inverted\"  # Specify the ranking method as \"inverted\"\n",
    "ranked_documents = get_ranked_documents(input_sentences, method_name, sample_query)  # Retrieve ranked documents\n",
    "print(\"Inverted Sample Word:\", sample_query)  # Print the sample query\n",
    "for inverted_text in ranked_documents:  # Loop through ranked documents\n",
    "    print(\"Sentence:\", inverted_text)  # Print each sentence with its index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF highest score of sample word: solve\n",
      "Sentence: 6\n",
      "Sentence: 4\n",
      "Sentence: 1\n",
      "Sentence: 2\n",
      "Sentence: 3\n",
      "Sentence: 5\n",
      "Sentence: 7\n",
      "Sentence: 8\n",
      "Sentence: 9\n",
      "Sentence: 10\n"
     ]
    }
   ],
   "source": [
    "# Sample query for testing\n",
    "sample_query = \"solve\"\n",
    "\n",
    "# Method for ranking documents\n",
    "method_name = \"TF-IDF\"\n",
    "\n",
    "# Retrieving ranked documents based on the TF-IDF method and the sample query\n",
    "ranked_documents = get_ranked_documents(input_sentences, method_name, sample_query)\n",
    "\n",
    "# Printing TF-IDF highest score of the sample word and the ranked sentences\n",
    "print(\"TF-IDF highest score of sample word:\", sample_query)\n",
    "for tfidf_text in ranked_documents:\n",
    "    print(\"Sentence:\", tfidf_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
