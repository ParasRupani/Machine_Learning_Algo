{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab8 Assignment Task PROG8245 - NLP Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Paras Rupani\n",
    "### ID: 8961758\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the discussed topic in class for tokenizers, stop-word removal, stemming/lemmatization, and POS Tagging. <br><br>\n",
    "Create **ONE** function, that takes as an input a string, and returns the output of a string after stemming/lemmatization.<br><br>\n",
    "**Kindly note that you are required to consider the POS Tag while doing your stemming or lemmatization step (you should use whatever is more suitable for this task)** <br><br>\n",
    "After creating the function, you need to run your function on 10 **Random** files from reuters corpus, an example of how to download and load a file of reuters corpus is below. <br><br>\n",
    "**Your 10 **Random** files should be retrieved by getting a random array of length 10 which picks numbers RANDOMLY from 0 to len(reuters.fileids()), then the elements retrieved will be your corpus.<br> <br>*You need to set your Seed to be Equal to the last 3 digits in your studentID.*<br>** If your ID is 8000888 then seed =888 <br>\n",
    "**You may need to tailor your task based on the dataset to remove some special characters.**\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step\n",
    "After finishing your code, run your code and save the result in a python dictionary, which would be of format:<br>\n",
    "{DocumentID: [List of Words], <br>\n",
    "...} <br>\n",
    "Save your python dictionary as a JSON file, or Pickle file. <br>\n",
    "A sample code for saving a python dictionary is available at the end of this notebook.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import nltk, pickle\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import  word_tokenize \n",
    "from nltk.corpus import stopwords, wordnet, reuters\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading various resources and corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/paras/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/paras/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/paras/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/paras/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('reuters')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 10788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"FRENCH FREE MARKET CEREAL EXPORT BIDS DETAILED\\n  French operators have requested licences\\n  to export 675,500 tonnes of maize, 245,000 tonnes of barley,\\n  22,000 tonnes of soft bread wheat and 20,000 tonnes of feed\\n  wheat at today's European Community tender, traders said.\\n      Rebates requested ranged from 127.75 to 132.50 European\\n  Currency Units a tonne for maize, 136.00 to 141.00 Ecus a tonne\\n  for barley and 134.25 to 141.81 Ecus for bread wheat, while\\n  rebates requested for feed wheat were 137.65 Ecus, they said.\\n  \\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to read specific file\n",
    "document_id = 'training/9865'\n",
    "text = reuters.raw(document_id) #reading a sample file from reuters\n",
    "\n",
    "print(\"Number of files:\",len(reuters.fileids())) #checking how many files are there\n",
    "\n",
    "text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7751, 8164, 9745, 8821, 5016,  759, 3060, 2724, 2385, 4594])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Random seed as last 3 digits of 8961758\n",
    "np.random.seed(758)\n",
    "random_files = np.random.choice(np.arange(len(reuters.fileids())), size=10, replace=False)\n",
    "\n",
    "random_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to perform PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocess(input_text):\n",
    "    special_chars = set(string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # POS Tagging\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "\n",
    "        elif tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "            \n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    # Tokenizing the input\n",
    "    tokens = word_tokenize(input_text)\n",
    "\n",
    "    # Removing stopwords and punctuation\n",
    "    clean_tokens = [token for token in tokens if token.lower() not in stop_words and token not in special_chars]\n",
    "\n",
    "    # Implementing Part-of-speech tagging   \n",
    "    pos_tags = pos_tag(clean_tokens)\n",
    "\n",
    "    # Lemmatization with POS tags\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcessing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use', 'advanced', 'laptop', 'perform', 'machine', 'learn', 'evaluation']\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "input_text = \"I am using an advanced laptop to perform machine learning evaluations\"\n",
    "print(nlp_preprocess(input_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving python dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store preprocessed text\n",
    "my_dict = {}\n",
    "\n",
    "for id in random_files:\n",
    "    # Retrieve the file ID\n",
    "    file_id = reuters.fileids()[id]\n",
    "    \n",
    "    # Extract the raw text content\n",
    "    text = reuters.raw(file_id)\n",
    "    \n",
    "    # Preprocess the text\n",
    "    preprocessed_text = nlp_preprocess(text)\n",
    "    \n",
    "    # Storing the preprocessed text\n",
    "    my_dict[file_id] = preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading, Writing the Corpus into a Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a file using Pickle\n",
    "with open('output.pkl', 'wb') as f:\n",
    "    pickle.dump(my_dict, f)\n",
    "\n",
    "    \n",
    "# Reading the pickle file:\n",
    "with open('output.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Content of 10 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/529: ['lt', 'FRANKLIN', 'UTILITIES', 'FUND', 'SETS', 'PAYOUT', 'Qtly', 'div', '14', 'ct', 'vs', '14', 'ct', 'prior', 'Pay', 'March', '13', 'Record', 'March', 'Two']\n",
      "\n",
      "training/5901: ['PAYLESS', 'CASHWAYS', 'INC', 'lt', 'PCI', '1ST', 'QTR', 'FEB', '28', 'NET', 'Shr', '10', 'ct', 'vs', 'seven', 'ct', 'Net', '3,501,000', 'v', '2,420,000', 'Sales', '332.7', 'mln', 'vs', '274.9', 'mln', 'Qtly', 'div', 'four', 'ct', 'vs', 'four', 'ct', 'prior', 'Pay', 'April', 'Six', 'Record', 'March', 'Six']\n",
      "\n",
      "training/8388: ['COMPUTER', 'DEVICES', 'INC', '4TH', 'QTR', 'Shr', 'loss', 'one', 'cnt', 'v', 'profit', 'one', 'cnt', 'Net', 'loss', '35,000', 'vs', 'profit', '42,000', 'Revs', '881,000', 'v', '1.3', 'mln', 'Year', 'Shr', 'profit', 'seven', 'ct', 'vs', 'profit', 'nine', 'ct', 'Net', 'profit', '291,000', 'vs', 'profit', '366,000', 'Revs', '4.4', 'mln', 'v', '5.9', 'mln', 'NOTE:1985', '4th', 'qtr', 'year', 'include', 'gain', '7,000', 'dlrs', '147,000', 'dlrs', 'respectivley', '1986', 'year', 'include', 'gain', '35,000', 'dlrs', 'tax', 'loss', 'carryforwards']\n",
      "\n",
      "training/6930: ['LIFESTYLE', 'lt', 'LIF', 'BOMBAY', 'AMEND', 'MERGER', 'AGREEMENT', 'Lifestyle', 'Restaurants', 'Inc', 'say', 'reduce', 'number', 'Bombay', 'Palace', 'Restaurants', 'inc', 'common', 'share', 'receive', 'previously', 'announce', 'merger', 'agreement', 'amend', 'deal', 'Lifestyle', 'shareholder', 'get', 'one', 'Bombay', 'share', 'six', 'instead', 'five', 'Lifestyle', 'share', 'amend', 'offer', 'Bombay', 'issue', '900,000', 'share', 'currently', '7.2', 'mln', 'dlrs', 'amendment', 'also', 'increase', 'cash', 'consideration', 'offer', 'Lifestyle', \"'s\", '13', 'pct', 'convertible', 'subordinated', 'debenture', '55', 'pct', 'principal', 'amount', '57.5', 'pct']\n",
      "\n",
      "training/1324: ['PANCANADIAN', 'SELL', 'NORTH', 'SEA', 'PROPERTIES', 'UNIT', 'WHITEHALL', 'CASH', 'PANCANADIAN', 'SELL', 'NORTH', 'SEA', 'PROPERTIES', 'UNIT', 'WHITEHALL', 'CASH']\n",
      "\n",
      "test/16188: ['U.K.', 'MONEY', 'MARKET', 'GIVEN', '75', 'MLN', 'STG', 'ASSISTANCE', 'Bank', 'England', 'say', 'provided', 'money', 'market', '75', 'mln', 'stg', 'help', 'morning', 'session', 'compare', 'Bank', \"'s\", 'estimate', 'system', 'would', 'face', 'shortage', 'around', '400', 'mln', 'stg', 'today', 'central', 'bank', 'buy', 'bank', 'bill', 'outright', 'comprise', 'two', 'mln', 'stg', 'band', 'two', '9-13/16', 'pct', '15', 'mln', 'stg', 'band', 'three', '9-3/4', 'pct', '58', 'mln', 'stg', 'band', 'three', '9-11/16', 'pct']\n",
      "\n",
      "training/10071: ['DENNISON', 'MANUFACTURING', 'lt', 'DSN', 'SELL', 'PAPER', 'UNIT', 'Dennison', 'Manufacturing', 'Co', 'say', 'sign', 'letter', 'intent', 'sell', 'Dunn', 'Paper', 'Co', 'subsidiary', 'James', 'River', 'Corp', 'lt', 'JR', 'undisclosed', 'amount', 'cash', 'result', 'first', 'quarter', 'charge', 'earnings', 'company', 'saiud', 'loss', 'sale', 'may', 'partly', 'reduce', 'contingent', 'payment', 'next', 'five', 'year', 'first', 'quarter', 'offset', 'gain', 'previously-announced', 'sale', 'Hygeia', 'Sciences', 'Inc', 'share', 'Dunn', 'sale', 'last', 'year', '65', 'mln', 'dlrs', 'Richmond', 'Va.', 'James', 'River', 'say', 'closing', 'expect', 'end', 'April', 'subject', 'approval', 'board', 'reach', 'satisfactory', 'labor', 'agreement']\n",
      "\n",
      "test/21051: ['SUPERMARKETS', 'GENERAL', 'lt', 'SGL', 'SELLS', '11', 'DRUG', 'STORES', 'Supermarkets', 'General', 'Corp', 'say', 'agree', 'sell', '11', 'super', 'drug', 'store', 'lt', 'F', 'Distributors', 'nine', 'exist', 'two', 'unopened', 'store', 'locate', 'Maryland', 'Virginia', 'upstate', 'New', 'York', 'operate', 'Pathmark', 'Super', 'Drug', 'trade', 'name', 'company', 'say', 'Terms', 'transaction', 'disclose', 'nine', 'exist', 'store', 'generate', 'approximately', '34.8', 'mln', 'dlrs', 'Supermarkets', 'General', \"'s\", 'total', 'sale', '2.9', 'billion', 'six-month', 'period', 'end', 'Aug', 'One', '1987', 'F', 'Distributors', 'operate', '42', 'discount', 'drug', 'store', 'Michigan', 'Ohio', 'Illinois', 'Indiana', 'Wisconsin']\n",
      "\n",
      "test/20364: ['ALLIANCE', 'FINANCIAL', 'CORP', 'lt', 'ALFL.O', '3RD', 'QTR', 'NET', 'Shr', '45', 'ct', 'vs', '61', 'ct', 'Net', '504,000', 'v', '683,000', 'Nine', 'mths', 'Shr', '1.83', 'dlrs', 'v', '2.42', 'dlrs', 'Net', '2,043,000', 'v', '2,183,000']\n",
      "\n",
      "training/12422: ['JAPAN', 'MINISTRY', 'ASKS', 'TRUST', 'BANKS', 'CUT', 'DLR', 'SALES', 'Finance', 'Ministry', 'ask', 'trust', 'bank', 'moderate', 'dollar', 'selling', 'trust', 'banking', 'source', 'say', 'Ministry', 'official', 'tell', 'Reuters', 'earlier', 'week', 'Ministry', 'recently', 'survey', 'foreign', 'exchange', 'transaction', 'institutional', 'investor', 'decline', 'say', 'whether', 'aim', 'moderate', 'dollar', 'sale', 'Dealers', 'say', 'institutional', 'investor', 'reluctant', 'sell', 'dollar', 'aggressively', 'today', 'partly', 'Ministry', 'monitor', 'One', 'senior', 'trust', 'bank', 'source', 'say', 'sympathize', 'Ministry', 'position', 'trust', 'bank', 'conduct', 'foreign', 'exchange', 'operation', 'accord', 'dictate', 'market', 'Bank', 'Japan', 'official', 'say', 'central', 'bank', 'approve', 'survey', 'long', 'use', 'forcefully', 'another', 'official', 'deny', 'local', 'press', 'report', 'central', 'bank', 'ask', 'investor', 'moderate', 'dollar', 'sale', '``', 'legally', 'authorize', \"''\", 'say', 'Bank', 'Japan', 'official', 'also', 'say', 'central', 'bank', 'renew', 'call', 'financial', 'institution', 'moderate', 'excessive', 'loan', 'purpose', 'land', 'security', 'investment', 'investment', 'threaten', 'cause', 'inflation', 'Bank', 'Japan', 'Governor', 'Satoshi', 'Sumita', 'previously', 'express', 'concern', 'excessive', 'investment', 'land', 'security', 'result', 'partly', 'ease', 'credit', 'condition']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the FileID and content\n",
    "for key, value in loaded_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
